{
  "pipe": [
    {
      "component": "tokenizer.spacy",
      "in": ["text"],
      "out": ["tokens"]
    },
    {
      "component": "tokenizer.chars",
      "in": ["tokens"],
      "out": ["chars"]
    },
    {
      "id": "tokens_vocab",
      "component": "vocab",
      "load": "./tmp/vocabs/ner.tokens.vocab.txt"
    },
    {
      "id": "tags_vocab",
      "component": "vocab",
      "load": "./tmp/vocabs/ner.tags.vocab.txt"
    },
    {
      "id": "chars_vocab",
      "component": "vocab",
      "load": "./tmp/vocabs/ner.chars.vocab.txt",
    },
    {
      "component": "ner",
      "load": "./tmp/models/ner",
      "init": {
        "tokens_vocab": "tokens_vocab",
        "tags_vocab": "tags_vocab",
        "chars_vocab": "chars_vocab",
      },
      "in": ["tokens", "chars"],
      "out": ["tags"]
    }
  ]
}