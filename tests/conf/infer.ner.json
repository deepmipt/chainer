{
  "pipe": [
    {
      "component": "tokenizer.spacy",
      "in": ["text"],
      "out": ["tokens"]
    },
    {
      "component": "tokenizer.chars",
      "in": ["tokens"],
      "out": ["chars"]
    },
    {
      "component": "ner",
      "load": "./tmp/models/ner",
      "init": {
        "tokens_vocab": {
          "component": "vocab",
          "load": "./tmp/vocabs/ner.tokens.vocab.txt"
        },
        "tags_vocab": {
          "component": "vocab",
          "load": "./tmp/vocabs/ner.tags.vocab.txt"
        },
        "chars_vocab": {
          "component": "vocab",
          "load": "./tmp/vocabs/ner.chars.vocab.txt"
        }
      },
      "in": ["tokens", "chars"],
      "out": ["tags"]
    }
  ]
}